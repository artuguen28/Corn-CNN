{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a proper Keras dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-25 18:42:18.822738: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-25 18:42:21.142580: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Keras folders and subfolders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "original_dataset_dir = \"dataset/corn_kenel_counting_dataset/corn_coco\"\n",
    "\n",
    "# Opening JSON file\n",
    "f_train = open(f'{original_dataset_dir}/corn_kernel_train.json')\n",
    "f_test = open(f'{original_dataset_dir}/corn_kernel_test.json')\n",
    "\n",
    "train_classes_mapper = json.load(f_train)\n",
    "test_classes_mapper = json.load(f_test)\n",
    "\n",
    "# def creating_folders(original_dataset_dir, classes_mapper):\n",
    "    \n",
    "#     parent = os.getcwd()\n",
    "    \n",
    "#     base_dir = f\"data\"\n",
    "#     os.mkdir(base_dir)\n",
    "    \n",
    "#     train_dir = os.path.join(base_dir, \"training\")\n",
    "#     os.mkdir(train_dir)\n",
    "#     test_dir = os.path.join(base_dir, \"testing\")\n",
    "#     os.mkdir(test_dir)\n",
    "#     valid_dir = os.path.join(base_dir, \"validation\")\n",
    "#     os.mkdir(valid_dir)\n",
    "    \n",
    "#     for c in range(len(classes_mapper[\"categories\"])):\n",
    "\n",
    "#         new_dir = os.path.join(train_dir, f\"{classes_mapper['categories'][c]['name']}\")\n",
    "#         os.mkdir(new_dir)\n",
    "        \n",
    "#     for c in range(len(classes_mapper[\"categories\"])):\n",
    "\n",
    "#         new_dir = os.path.join(test_dir, f\"{classes_mapper['categories'][c]['name']}\")\n",
    "#         os.mkdir(new_dir)\n",
    "        \n",
    "#     for c in range(len(classes_mapper[\"categories\"])):\n",
    "\n",
    "#         new_dir = os.path.join(valid_dir, f\"{classes_mapper['categories'][c]['name']}\")\n",
    "#         os.mkdir(new_dir)\n",
    "\n",
    "def creating_folders( classes_mapper):\n",
    "    \n",
    "    parent = os.getcwd()\n",
    "    \n",
    "    base_dir = f\"input\"\n",
    "    os.mkdir(base_dir)\n",
    "\n",
    "    \n",
    "    for c in range(len(classes_mapper[\"categories\"])):\n",
    "\n",
    "        new_dir = os.path.join(base_dir, f\"{classes_mapper['categories'][c]['name']}\")\n",
    "        os.mkdir(new_dir)\n",
    "\n",
    "\n",
    "creating_folders(train_classes_mapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organizing images in the input folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "def creating_image_dataset(classes_mapper, base_dir, original_dataset_dir, index=0):\n",
    "    \n",
    "    ear_classes = {\"26560\": \"Kernel\", \n",
    "           \"26561\": \"Diseased Kernel\", \n",
    "           \"26562\": \"Incomplete Kernel\", \n",
    "           \"26563\": \"Baren Tip\", \n",
    "           \"33761\": \"Kernel Area\"}\n",
    "\n",
    "    # Iterate through the files\n",
    "    for i in range(len(classes_mapper[\"images\"])):\n",
    "        \n",
    "        for a in range(len(classes_mapper[\"annotations\"])):\n",
    " \n",
    "            if classes_mapper[\"images\"][i][\"id\"] == classes_mapper[\"annotations\"][a][\"image_id\"]:\n",
    "\n",
    "                image = cv2.imread(f\"{original_dataset_dir}/{classes_mapper['images'][i]['file_name']}\")\n",
    "            \n",
    "                x, y, width, height = list(map(int, classes_mapper[\"annotations\"][a][\"bbox\"]))\n",
    "                \n",
    "                cropped_image = image[y: (height + y), x: (width + x)]\n",
    "\n",
    "                id_annotation = int(classes_mapper[\"annotations\"][a]['id']) + index\n",
    "                \n",
    "                label = ear_classes[str(classes_mapper[\"annotations\"][a][\"category_id\"])]\n",
    "                print(label, id_annotation)\n",
    "\n",
    "                if 1 <= id_annotation < 10:\n",
    "                    cv2.imwrite(f\"{base_dir}/{label}/0000{id_annotation}.jpg\", cropped_image)\n",
    "\n",
    "                elif 10 <= id_annotation < 100:\n",
    "                    cv2.imwrite(f\"{base_dir}/{label}/000{id_annotation}.jpg\", cropped_image)\n",
    "\n",
    "                elif 100 <= id_annotation < 1000:\n",
    "                    cv2.imwrite(f\"{base_dir}/{label}/00{id_annotation}.jpg\", cropped_image)\n",
    "\n",
    "                elif 1000 <= id_annotation < 10000:\n",
    "                    cv2.imwrite(f\"{base_dir}/{label}/0{id_annotation}.jpg\", cropped_image)\n",
    "\n",
    "                else:\n",
    "                    cv2.imwrite(f\"{base_dir}/{label}/{id_annotation}.jpg\", cropped_image)\n",
    "                    \n",
    "    print(\"Finished\")\n",
    "    \n",
    "creating_image_dataset(\n",
    "    test_classes_mapper, \n",
    "    \"input\", \n",
    "    original_dataset_dir)\n",
    "\n",
    "creating_image_dataset(\n",
    "    train_classes_mapper, \n",
    "    \"input\", \n",
    "    original_dataset_dir,\n",
    "    index = 16010)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import splitfolders\n",
    "\n",
    "splitfolders.ratio(\"input\", output=\"data\",\n",
    "    seed=42, ratio=(.7, .1, .2), group_prefix=None, move=False) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the input data\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    \"data/train\",\n",
    "    target_size=(224, 224),\n",
    "    batch_size=20,\n",
    "    class_mode='categorical',\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    \"data/val\",\n",
    "    target_size=(224, 224),\n",
    "    batch_size=20,\n",
    "    class_mode='categorical',\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50655 images belonging to 5 classes.\n",
      "Found 7235 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "# Downloading the model\n",
    "from keras.applications.mobilenet_v2 import MobileNetV2\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "model = MobileNetV2(weights='imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customizing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras import Model\n",
    "from keras.layers import Dense\n",
    "import keras\n",
    "\n",
    "corn_output = Dense(5, activation='softmax')\n",
    "corn_output = corn_output(model.layers[-2].output)\n",
    "corn_input = model.input\n",
    "corn_model = Model(inputs=corn_input, outputs=corn_output)\n",
    "\n",
    "for layer in corn_model.layers[:-1]:\n",
    "    layer.trainable\n",
    "\n",
    "corn_model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "callbacks_list = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='acc',\n",
    "        patience=10,\n",
    "    ),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='my_model.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "    ),\n",
    "    tf.keras.callbacks.CSVLogger(\n",
    "        \"Log\", \n",
    "        separator=\",\", \n",
    "        append=False\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-25 20:00:03.606429: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n",
      "2023-07-25 20:00:13.785362: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-07-25 20:00:19.727472: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 79.09MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-07-25 20:00:19.829024: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 98.18MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits' defined at (most recent call last):\n    File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"/usr/lib/python3/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"/usr/lib/python3/dist-packages/traitlets/config/application.py\", line 846, in launch_instance\n      app.start()\n    File \"/usr/lib/python3/dist-packages/ipykernel/kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"/usr/lib/python3/dist-packages/tornado/platform/asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.10/asyncio/base_events.py\", line 600, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1896, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/lib/python3/dist-packages/ipykernel/kernelbase.py\", line 461, in dispatch_queue\n      await self.process_one()\n    File \"/usr/lib/python3/dist-packages/ipykernel/kernelbase.py\", line 450, in process_one\n      await dispatch(*args)\n    File \"/usr/lib/python3/dist-packages/ipykernel/kernelbase.py\", line 357, in dispatch_shell\n      await result\n    File \"/usr/lib/python3/dist-packages/ipykernel/kernelbase.py\", line 652, in execute_request\n      reply_content = await reply_content\n    File \"/usr/lib/python3/dist-packages/ipykernel/ipkernel.py\", line 353, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/usr/lib/python3/dist-packages/ipykernel/zmqshell.py\", line 532, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py\", line 2914, in run_cell\n      result = self._run_cell(\n    File \"/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py\", line 2960, in _run_cell\n      return runner(coro)\n    File \"/usr/lib/python3/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py\", line 3185, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py\", line 3377, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py\", line 3457, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_45/3273191676.py\", line 1, in <module>\n      corn_model.fit(\n    File \"/home/arthur/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/arthur/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1742, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/home/arthur/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1338, in train_function\n      return step_function(self, iterator)\n    File \"/home/arthur/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1322, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/arthur/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1303, in run_step\n      outputs = model.train_step(data)\n    File \"/home/arthur/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1081, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/arthur/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1139, in compute_loss\n      return self.compiled_loss(\n    File \"/home/arthur/.local/lib/python3.10/site-packages/keras/src/engine/compile_utils.py\", line 265, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/home/arthur/.local/lib/python3.10/site-packages/keras/src/losses.py\", line 142, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"/home/arthur/.local/lib/python3.10/site-packages/keras/src/losses.py\", line 268, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/arthur/.local/lib/python3.10/site-packages/keras/src/losses.py\", line 2354, in sparse_categorical_crossentropy\n      return backend.sparse_categorical_crossentropy(\n    File \"/home/arthur/.local/lib/python3.10/site-packages/keras/src/backend.py\", line 5762, in sparse_categorical_crossentropy\n      res = tf.nn.sparse_softmax_cross_entropy_with_logits(\nNode: 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits'\nlogits and labels must have the same first dimension, got logits shape [20,5] and labels shape [100]\n\t [[{{node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_train_function_52130]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_45/3273191676.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m corn_model.fit(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits' defined at (most recent call last):\n    File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"/usr/lib/python3/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"/usr/lib/python3/dist-packages/traitlets/config/application.py\", line 846, in launch_instance\n      app.start()\n    File \"/usr/lib/python3/dist-packages/ipykernel/kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"/usr/lib/python3/dist-packages/tornado/platform/asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.10/asyncio/base_events.py\", line 600, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1896, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/lib/python3/dist-packages/ipykernel/kernelbase.py\", line 461, in dispatch_queue\n      await self.process_one()\n    File \"/usr/lib/python3/dist-packages/ipykernel/kernelbase.py\", line 450, in process_one\n      await dispatch(*args)\n    File \"/usr/lib/python3/dist-packages/ipykernel/kernelbase.py\", line 357, in dispatch_shell\n      await result\n    File \"/usr/lib/python3/dist-packages/ipykernel/kernelbase.py\", line 652, in execute_request\n      reply_content = await reply_content\n    File \"/usr/lib/python3/dist-packages/ipykernel/ipkernel.py\", line 353, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/usr/lib/python3/dist-packages/ipykernel/zmqshell.py\", line 532, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py\", line 2914, in run_cell\n      result = self._run_cell(\n    File \"/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py\", line 2960, in _run_cell\n      return runner(coro)\n    File \"/usr/lib/python3/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py\", line 3185, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py\", line 3377, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py\", line 3457, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_45/3273191676.py\", line 1, in <module>\n      corn_model.fit(\n    File \"/home/arthur/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/arthur/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1742, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/home/arthur/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1338, in train_function\n      return step_function(self, iterator)\n    File \"/home/arthur/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1322, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/arthur/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1303, in run_step\n      outputs = model.train_step(data)\n    File \"/home/arthur/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1081, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/arthur/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1139, in compute_loss\n      return self.compiled_loss(\n    File \"/home/arthur/.local/lib/python3.10/site-packages/keras/src/engine/compile_utils.py\", line 265, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/home/arthur/.local/lib/python3.10/site-packages/keras/src/losses.py\", line 142, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"/home/arthur/.local/lib/python3.10/site-packages/keras/src/losses.py\", line 268, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/arthur/.local/lib/python3.10/site-packages/keras/src/losses.py\", line 2354, in sparse_categorical_crossentropy\n      return backend.sparse_categorical_crossentropy(\n    File \"/home/arthur/.local/lib/python3.10/site-packages/keras/src/backend.py\", line 5762, in sparse_categorical_crossentropy\n      res = tf.nn.sparse_softmax_cross_entropy_with_logits(\nNode: 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits'\nlogits and labels must have the same first dimension, got logits shape [20,5] and labels shape [100]\n\t [[{{node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_train_function_52130]"
     ]
    }
   ],
   "source": [
    "corn_model.fit(\n",
    "    train_generator,\n",
    "    epochs=100,\n",
    "    verbose=2,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=callbacks_list\n",
    ")\n",
    "\n",
    "model.save('corn_classification.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Corn-CNN",
   "language": "python",
   "name": "corn-cnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
